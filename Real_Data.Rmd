---
title: "Real Data"
author: "Ruiyang Wu"
date: "4/14/2019"
output: pdf_document
header-includes:
- \usepackage{graphicx,hhline}
- \usepackage{amssymb}
- \usepackage{amsfonts}
- \usepackage{amscd}
- \usepackage{amsthm}
- \usepackage{amsmath}
- \usepackage{mathabx}
- \usepackage{multirow}
---

\newcommand{\rE}{\mathrm{E}}
\newcommand{\var}{\mathrm{Var}}
\newcommand{\cov}{\mathrm{Cov}}
\newcommand{\ba}{\mbox{\boldmath $a$}}
\newcommand{\be}{\mbox{\boldmath $e$}}
\newcommand{\bx}{\mbox{\boldmath $x$}}
\newcommand{\bX}{\mbox{\boldmath $X$}}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, eval = FALSE)
library(DAP)
library(rda)
library(MASS)
library(ggplot2)
library(dsda)
source('datagen.R')
source('QDAp.R')
source('QDAp_2.R')
```

### 1.

Let's compare the breast cancer data. $p=9$, $n=699$. Repeat $m=100$ times for average test error. 

Read the data:
```{r}
df_bc <- NULL
```
```{r}
bc <- read.csv('breast-cancer-wisconsin.data', header = FALSE)
y <- bc[,11]
id0 <- which(y == 2)
id1 <- which(y == 4)
y[id0] <- 0
y[id1] <- 1
x <- data.matrix(bc[,2:10])
#x=scale(x)
```

Set seed and replicates, split data into training test:
```{r}
m <- 100
set.seed(201)
per <- 0.6
n0 <- length(id0)
n1 <- length(id1)
id <- list()
for (i in 1:m) {
  id0tr <- sample(id0, round(per*n0))
  id1tr <- sample(id1, round(per*n1))
  id[[i]] <- c(id0tr, id1tr)
}
```

Our method:
```{r}
y[which(y == 2)] <- 0
pe <- rep(0, m)
for (i in 1:m) {
  ynew <- QDAp(x[id[[i]],], as.factor(y[id[[i]]]), x[-id[[i]],], lambda = 0, iter = 1)$class
  pe[i] <- mean(ynew != y[-id[[i]]])
}
df_bc <- rbind(df_bc, data.frame(Methods = "Our method", Training.Percentage = per, Prediction.Error = mean(pe), ci = sd(pe)*qt(0.975, m - 1)/sqrt(m)))
```

Our method(threshold):
```{r eval=FALSE}
pe <- rep(0,m)
y[which(y == 2)] <- 0
for(i in 1:m){
  set.seed(10)
  l <- length(id[[i]])
  cvind <- sample(1:l)
  lambda <- exp(-(3:10))
  cve <- rep(0, 8)
  for(j in 1:8){
    for(k in 1:5){
      ynew <- QDAp(x[id[[i]][cvind][-((floor((k-1)*l/5)+1):floor(k*l/5))],],as.factor(y[id[[i]][cvind][-((floor((k-1)*l/5)+1):floor(k*l/5))]]),x[id[[i]][cvind][(floor((k-1)*l/5)+1):floor(k*l/5)],],lambda=lambda[j], method = "Thresholding")$class
      cve[j] <- cve[j]+sum(ynew!=y[id[[i]][cvind][(floor((k-1)*l/5)+1):floor(k*l/5)]])
    }
    cve[j] <- cve[j]/l
  }
  lambda.cv <- lambda[which.min(cve)]
  print(lambda.cv)
  ynew=QDAp(x[id[[i]],], as.factor(y[id[[i]]]), x[-id[[i]],], lambda=lambda.cv, method = "Thresholding")$class
  pe[i]=mean(ynew!=y[-id[[i]]])
}
mean(pe)
```

Our method(regularized):
```{r eval=FALSE}
s=0
for(i in 1:m){
  ynew=QDAp_2(x[id[[i]],],as.factor(y[id[[i]]]),x[-id[[i]],],lambda=0.001,iter = 1)
  s=s+mean(ynew!=y[-id[[i]]])
}
s/m
```

LDA:
```{r warning=FALSE}
y[which(y==2)]=0
pe=rep(0,m)
for(i in 1:m){
  fit.lda<-lda(x[id[[i]],],as.factor(y[id[[i]]]))
  ynew=predict(fit.lda,x[-id[[i]],])$class
  pe[i]=mean(ynew!=y[-id[[i]]])
}
df_bc=rbind(df_bc,data.frame(Methods="LDA",Training.Percentage=per,Prediction.Error=mean(pe),ci=sd(pe)*qt(0.975,m-1)/sqrt(m)))
```

Direct Sparse Discriminant Analysis (QING MAI, HUI ZOU, MING YUAN):
```{r}
y[which(y==2)]=0
pe=rep(0,m)
for(i in 1:m){
  pe[i]=dsda(as.matrix(x[id[[i]],]),y[id[[i]]],as.matrix(x[-id[[i]],]),y[-id[[i]]])$error
}
df_bc=rbind(df_bc,data.frame(Methods="SLDA",Training.Percentage=per,Prediction.Error=mean(pe),ci=sd(pe)*qt(0.975,m-1)/sqrt(m)))
```

QDA:
```{r}
y[which(y==2)]=0
pe=rep(0,m)
for(i in 1:m){
  fit.qda=qda(x[id[[i]],],as.factor(y[id[[i]]]))
  ynew=predict(fit.qda,x[-id[[i]],])$class
  pe[i]=mean(ynew!=y[-id[[i]]])
}
df_bc=rbind(df_bc,data.frame(Methods="QDA",Training.Percentage=per,Prediction.Error=mean(pe),ci=sd(pe)*qt(0.975,m-1)/sqrt(m)))
```

Sparse quadratic classification rules via linear dimension reduction (Irina Gaynanova and Tianying Wang):
```{r results='hide'}
y[which(y==0)]=2
pe=rep(0,m)
for(i in 1:m){
  pe[i]=apply_DAP(x[id[[i]],],y[id[[i]]],x[-id[[i]],],y[-id[[i]]])$error
}
df_bc=rbind(df_bc,data.frame(Methods="SQDA",Training.Percentage=per,Prediction.Error=mean(pe),ci=sd(pe)*qt(0.975,m-1)/sqrt(m)))
```

RDA:
```{r results='hide'}
y[which(y==0)]=2
pe=rep(0,m)
for(i in 1:m){
  fit.rda<-rda(t(x[id[[i]],]),as.factor(y[id[[i]]]))
  err<-rda.cv(fit.rda,t(x[id[[i]],]),as.factor(y[id[[i]]]))$cv.err
  par<-arrayInd(which.min(err),dim(err))
  fit.rda<-rda(t(x[id[[i]],]),as.factor(y[id[[i]]]),alpha = seq(0, 0.99, len=10)[par[1]],delta = seq(0, 3, len=10)[par[2]])
  ynew=predict(fit.rda,t(x[id[[i]],]),as.factor(y[id[[i]]]),t(x[-id[[i]],]),type="class")
  pe[i]=mean(ynew!=y[-id[[i]]])
}
df_bc=rbind(df_bc,data.frame(Methods="RDA",Training.Percentage=per,Prediction.Error=mean(pe),ci=sd(pe)*qt(0.975,m-1)/sqrt(m)))
```

Plot CI:
```{r}
pdf("Breast_Cancer.pdf")
pd <- position_dodge(0.01)
ggplot(df_bc, aes(x=Training.Percentage, y=Prediction.Error, colour=Methods)) + 
    geom_errorbar(aes(ymin=Prediction.Error-ci, ymax=Prediction.Error+ci), width=.02, position=pd) +
    geom_line(position=pd) +
    geom_point(position=pd)
dev.off()
```

### 2.

Compare Ultrasonic flowmeter data. $p=36$, $n=87$. Repeat $m=100$ times for average test error.

Read the data:
```{r}
df_FM=NULL
```
```{r}
FM=read.table('Meter A.data')
y=FM[,37]
id0=which(y==2)
id1=which(y==1)
y[id0]=0
x=data.matrix(FM[,1:36])
#x=scale(x)
```

Set seed and replicates, split data into training test:
```{r}
m=100
set.seed(301)
per=0.40
n0=length(id0)
n1=length(id1)
id=list()
for(i in 1:m){
  id0tr=sample(id0,round(per*n0))
  id1tr=sample(id1,round(per*n1))
  id[[i]]=c(id0tr,id1tr)
}
```

Our method:
```{r}
y[which(y==2)]=0
pe=rep(0,m)
for(i in 1:m){
  ynew=QDAp(x[id[[i]],],as.factor(y[id[[i]]]),x[-id[[i]],],lambda=0,iter = 1)$class
  pe[i]=mean(ynew!=y[-id[[i]]])
}
df_FM=rbind(df_FM,data.frame(Methods="Our method",Training.Percentage=per,Prediction.Error=mean(pe),ci=sd(pe)*qt(0.975,m-1)/sqrt(m)))
```

Our method(threshold):
```{r eval=FALSE}
s=0
for(i in 1:m){
  ynew=QDAp(x[id[[i]],],as.factor(y[id[[i]]]),x[-id[[i]],],lambda=0.0008,iter = 2)
  s=s+mean(ynew!=y[-id[[i]]])
}
s/m
```

Our method(penalized):
```{r eval=FALSE}
s=0
for(i in 1:m){
  ynew=QDAp_2(x[id[[i]],],as.factor(y[id[[i]]]),x[-id[[i]],],lambda=0.0014,iter = 1)
  s=s+mean(ynew!=y[-id[[i]]])
}
s/m
```

LDA:
```{r warning=FALSE}
y[which(y==2)]=0
pe=rep(0,m)
for(i in 1:m){
  fit.lda<-lda(x[id[[i]],],as.factor(y[id[[i]]]))
  ynew=predict(fit.lda,x[-id[[i]],])$class
  pe[i]=mean(ynew!=y[-id[[i]]])
}
df_FM=rbind(df_FM,data.frame(Methods="LDA",Training.Percentage=per,Prediction.Error=mean(pe),ci=sd(pe)*qt(0.975,m-1)/sqrt(m)))
```

Direct Sparse Discriminant Analysis (QING MAI, HUI ZOU, MING YUAN):
```{r}
y[which(y==2)]=0
pe=rep(0,m)
for(i in 1:m){
  pe[i]=dsda(as.matrix(x[id[[i]],]),y[id[[i]]],as.matrix(x[-id[[i]],]),y[-id[[i]]])$error
}
df_FM=rbind(df_FM,data.frame(Methods="SLDA",Training.Percentage=per,Prediction.Error=mean(pe),ci=sd(pe)*qt(0.975,m-1)/sqrt(m)))
```

QDA:  
Does not work.

Sparse quadratic classification rules via linear dimension reduction (Irina Gaynanova and Tianying Wang):
```{r results='hide'}
y[which(y==0)]=2
pe=rep(0,m)
for(i in 1:m){
  pe[i]=apply_DAP(x[id[[i]],],y[id[[i]]],x[-id[[i]],],y[-id[[i]]])$error
}
df_FM=rbind(df_FM,data.frame(Methods="SQDA",Training.Percentage=per,Prediction.Error=mean(pe),ci=sd(pe)*qt(0.975,m-1)/sqrt(m)))
```

RDA:
```{r results='hide'}
y[which(y==0)]=2
pe=rep(0,m)
for(i in 1:m){
  fit.rda<-rda(t(x[id[[i]],]),as.factor(y[id[[i]]]))
  err<-rda.cv(fit.rda,t(x[id[[i]],]),as.factor(y[id[[i]]]))$cv.err
  par<-arrayInd(which.min(err),dim(err))
  fit.rda<-rda(t(x[id[[i]],]),as.factor(y[id[[i]]]),alpha = seq(0, 0.99, len=10)[par[1]],delta = seq(0, 3, len=10)[par[2]])
  ynew=predict(fit.rda,t(x[id[[i]],]),as.factor(y[id[[i]]]),t(x[-id[[i]],]),type="class")
  pe[i]=mean(ynew!=y[-id[[i]]])
}
df_FM=rbind(df_FM,data.frame(Methods="RDA",Training.Percentage=per,Prediction.Error=mean(pe),ci=sd(pe)*qt(0.975,m-1)/sqrt(m)))
```

Plot CI:
```{r}
pdf("Flowmeter.pdf")
pd <- position_dodge(0.01)
ggplot(df_FM, aes(x=Training.Percentage, y=Prediction.Error, colour=Methods)) + 
    geom_errorbar(aes(ymin=Prediction.Error-ci, ymax=Prediction.Error+ci), width=.02, position=pd) +
    geom_line(position=pd) +
    geom_point(position=pd)
dev.off()
```

### 3.

Heart Disease data.  $p=13$, $n=303$. Repeat $m=100,300$ times for average test error.

Read the data:
```{r}
df_HD=NULL
```
```{r}
HD=read.csv('heart.csv')
y=HD[,14]
id0=which(y==0)
id1=which(y==1)
x=data.matrix(HD[,1:13])
#x=scale(x)
```

Set seed and replicates, split data into training test:
```{r}
m=300
set.seed(401)
per=0.8
n0=length(id0)
n1=length(id1)
id=list()
for(i in 1:m){
  id0tr=sample(id0,round(per*n0))
  id1tr=sample(id1,round(per*n1))
  id[[i]]=c(id0tr,id1tr)
}
```

Our method:
```{r}
y[which(y==2)]=0
pe=rep(0,m)
for(j in 1:4)
for(i in 1:m){
  ynew=QDAp(x[id[[i]],],as.factor(y[id[[i]]]),x[-id[[i]],],lambda=0,iter = 1)$class
  pe[i]=mean(ynew!=y[-id[[i]]])
}
df_HD=rbind(df_HD,data.frame(Methods="Our method",Training.Percentage=per,Prediction.Error=mean(pe),ci=sd(pe)*qt(0.975,m-1)/sqrt(m)))
```

Our method(threshold):
```{r eval=FALSE}
s=0
for(i in 1:m){
  ynew=QDAp(x[id[[i]],],as.factor(y[id[[i]]]),x[-id[[i]],],lambda=0.025,iter = 1)
  s=s+mean(ynew!=y[-id[[i]]])
}
s/m
```

Our method(regularized):
```{r eval=FALSE}
s=0
for(i in 1:m){
  ynew=QDAp_2(x[id[[i]],],as.factor(y[id[[i]]]),x[-id[[i]],],lambda=0.001,iter = 1)
  s=s+mean(ynew!=y[-id[[i]]])
}
s/m
```

LDA:
```{r warning=FALSE}
y[which(y==2)]=0
pe=rep(0,m)
for(i in 1:m){
  fit.lda<-lda(x[id[[i]],],as.factor(y[id[[i]]]))
  ynew=predict(fit.lda,x[-id[[i]],])$class
  pe[i]=mean(ynew!=y[-id[[i]]])
}
df_HD=rbind(df_HD,data.frame(Methods="LDA",Training.Percentage=per,Prediction.Error=mean(pe),ci=sd(pe)*qt(0.975,m-1)/sqrt(m)))
```

Direct Sparse Discriminant Analysis (QING MAI, HUI ZOU, MING YUAN):
```{r}
y[which(y==2)]=0
pe=rep(0,m)
for(i in 1:m){
  pe[i]=dsda(as.matrix(x[id[[i]],]),y[id[[i]]],as.matrix(x[-id[[i]],]),y[-id[[i]]])$error
}
df_HD=rbind(df_HD,data.frame(Methods="SLDA",Training.Percentage=per,Prediction.Error=mean(pe),ci=sd(pe)*qt(0.975,m-1)/sqrt(m)))
```

QDA:
```{r}
y[which(y==2)]=0
pe=rep(0,m)
for(i in 1:m){
  fit.qda=qda(x[id[[i]],],as.factor(y[id[[i]]]))
  ynew=predict(fit.qda,x[-id[[i]],])$class
  pe[i]=mean(ynew!=y[-id[[i]]])
}
df_HD=rbind(df_HD,data.frame(Methods="QDA",Training.Percentage=per,Prediction.Error=mean(pe),ci=sd(pe)*qt(0.975,m-1)/sqrt(m)))
```

Sparse quadratic classification rules via linear dimension reduction (Irina Gaynanova and Tianying Wang):
```{r results='hide'}
y[which(y==0)]=2
pe=rep(0,m)
for(i in 1:m){
  pe[i]=apply_DAP(x[id[[i]],],y[id[[i]]],x[-id[[i]],],y[-id[[i]]])$error
}
df_HD=rbind(df_HD,data.frame(Methods="SQDA",Training.Percentage=per,Prediction.Error=mean(pe),ci=sd(pe)*qt(0.975,m-1)/sqrt(m)))
```

RDA:
```{r results='hide'}
y[which(y==0)]=2
pe=rep(0,m)
for(i in 1:m){
  fit.rda<-rda(t(x[id[[i]],]),as.factor(y[id[[i]]]))
  err<-rda.cv(fit.rda,t(x[id[[i]],]),as.factor(y[id[[i]]]))$cv.err
  par<-arrayInd(which.min(err),dim(err))
  fit.rda<-rda(t(x[id[[i]],]),as.factor(y[id[[i]]]),alpha = seq(0, 0.99, len=10)[par[1]],delta = seq(0, 3, len=10)[par[2]])
  ynew=predict(fit.rda,t(x[id[[i]],]),as.factor(y[id[[i]]]),t(x[-id[[i]],]),type="class")
  pe[i]=mean(ynew!=y[-id[[i]]])
}
df_HD=rbind(df_HD,data.frame(Methods="RDA",Training.Percentage=per,Prediction.Error=mean(pe),ci=sd(pe)*qt(0.975,m-1)/sqrt(m)))
```

Plot CI:
```{r}
pdf("Heart_Disease.pdf")
pd <- position_dodge(0.01)
ggplot(df_HD, aes(x=Training.Percentage, y=Prediction.Error, colour=Methods)) + 
    geom_errorbar(aes(ymin=Prediction.Error-ci, ymax=Prediction.Error+ci), width=.02, position=pd) +
    geom_line(position=pd) +
    geom_point(position=pd)
dev.off()
```
